{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-testimony",
   "metadata": {},
   "source": [
    "First, I started off my journey with parsing the data properly.\n",
    "The code I used to parse the data can be viewed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"all_sentiment_shuffled.txt\", \"r\", encoding=\"utf8\")\n",
    "x = []\n",
    "y = []\n",
    "first_column = []\n",
    "third_column = []\n",
    "for num, line in enumerate(lines):\n",
    "    a = line.rstrip(\"\\n\").split(\" \", 3)\n",
    "    if a[1] == \"neg\":\n",
    "        y.append(0)\n",
    "    elif a[1] == \"pos\":\n",
    "        y.append(1)\n",
    "    x.append(a[0] + \" \" + a[2] + \" \" + a[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-australia",
   "metadata": {},
   "source": [
    "If I want to not care about the first column and third column in the data, I can replace the 12th line with the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.append(a[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-silicon",
   "metadata": {},
   "source": [
    "So it can be re-executed as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"all_sentiment_shuffled.txt\", \"r\", encoding=\"utf8\")\n",
    "x = []\n",
    "y = []\n",
    "first_column = []\n",
    "third_column = []\n",
    "for num, line in enumerate(lines):\n",
    "    a = line.rstrip(\"\\n\").split(\" \", 3)\n",
    "    if a[1] == \"neg\":\n",
    "        y.append(0)\n",
    "    elif a[1] == \"pos\":\n",
    "        y.append(1)\n",
    "    x.append(a[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-rainbow",
   "metadata": {},
   "source": [
    "Data is splitted to intended proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-painting",
   "metadata": {},
   "source": [
    "Then, we group the positive labels and negative labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_x_train = []\n",
    "neg_x_train = []\n",
    "for i, j in zip(x_train, y_train):\n",
    "    if j == 0:\n",
    "        neg_x_train.append(i)\n",
    "    elif j == 1:\n",
    "        pos_x_train.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-detail",
   "metadata": {},
   "source": [
    "Then we work out their posterior probabilities with the following two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_pos_x = len(pos_x_train) / (len(pos_x_train) + len(neg_x_train))\n",
    "posterior_neg_x = len(neg_x_train) / (len(pos_x_train) + len(neg_x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-thanks",
   "metadata": {},
   "source": [
    "Then, we can leverage sklearn to perform feature extraction phase with the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 1),  # to use bigrams ngram_range=(2,2)\n",
    "                                   stop_words='english',\n",
    "                                   analyzer='word')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-russian",
   "metadata": {},
   "source": [
    "If we want to get the form of them where TF-IDF algorithms had run. Then we can likewise the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_vectorizer = TfidfVectorizer(ngram_range=(1, 1),  # to use bigrams ngram_range=(2,2)\n",
    "                                  stop_words='english',\n",
    "                                  analyzer='word')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-leadership",
   "metadata": {},
   "source": [
    "These two objects do all for us. The only thing that remains for us is to change the parameters to see the varying results when they are plugged into our Naive Bayes implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-gallery",
   "metadata": {},
   "source": [
    "I've implemented Naive Bayes as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defined-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(l_count_vectorizer):\n",
    "    pos_x_train_ar = l_count_vectorizer.fit_transform(pos_x_train).toarray()\n",
    "    pos_indices = l_count_vectorizer.vocabulary_\n",
    "    pos_freq = np.sum(pos_x_train_ar, axis=0)\n",
    "    sum_pos_freq = np.sum(pos_freq)\n",
    "    neg_x_train_ar = l_count_vectorizer.fit_transform(neg_x_train).toarray()\n",
    "    neg_indices = l_count_vectorizer.vocabulary_\n",
    "    neg_freq = np.sum(neg_x_train_ar, axis=0)\n",
    "    sum_neq_freq = np.sum(neg_freq)\n",
    "    alpha = 1\n",
    "    predictions = []\n",
    "    for r in x_test:\n",
    "        row = l_count_vectorizer.fit_transform([r]).toarray()[0]\n",
    "        pos_prob = math.log(posterior_pos_x)\n",
    "        neg_prob = math.log(posterior_neg_x)\n",
    "        for vocab, freq in zip(l_count_vectorizer.vocabulary_.keys(), row):\n",
    "            if vocab in pos_indices:\n",
    "                pos_prob += math.log((pos_freq[pos_indices[vocab]] + alpha) / (sum_pos_freq + alpha * len(pos_freq))) * freq\n",
    "            else:\n",
    "                pos_prob += math.log(alpha / (sum_pos_freq + alpha * len(pos_freq))) * freq\n",
    "\n",
    "            if vocab in neg_indices:\n",
    "                neg_prob += math.log((neg_freq[neg_indices[vocab]] + alpha) / (sum_neq_freq + alpha * len(neg_freq))) * freq\n",
    "            else:\n",
    "                neg_prob += math.log(alpha / (sum_neq_freq + alpha * len(neg_freq))) * freq\n",
    "\n",
    "        if pos_prob > neg_prob:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-basket",
   "metadata": {},
   "source": [
    "I can get predictions from my naive bayes implementation as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = naive_bayes(count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_predictions = naive_bayes(tfid_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-structure",
   "metadata": {},
   "source": [
    "We can assess the results by means of the functions provided by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, tfid_predictions))\n",
    "print(accuracy_score(y_test, tfid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-nowhere",
   "metadata": {},
   "source": [
    "Now let's dissect the gotten results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-criticism",
   "metadata": {},
   "source": [
    "Let's start with the case in which we benefit from only the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count-vectorizer\n",
    "[[387  90]\n",
    " [114 409]]\n",
    "accuracy_rate = 0.796\n",
    "\n",
    "# tf-idf vectorizer\n",
    "[[396  81]\n",
    " [ 90 433]]\n",
    "accuracy_rate = 0.829"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-reasoning",
   "metadata": {},
   "source": [
    "The takeaway from the results above is that TF-IDF increases the success rate of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-combining",
   "metadata": {},
   "source": [
    "Let's see what happens if we set the n_grams to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count-vectorizer\n",
    "[[351 117]\n",
    " [155 377]]\n",
    "accuracy_rate = 0.728\n",
    "\n",
    "# tf-idf vectorizer\n",
    "[[400  68]\n",
    " [217 315]]\n",
    "accuracy_rate = 0.715"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-cursor",
   "metadata": {},
   "source": [
    "It reduces the success rate for both of them, notably the TF-IDF vectorizer is affected mostly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-thinking",
   "metadata": {},
   "source": [
    "If we use unigrams and bigrams together, then we'll get the best estimation result as can be verified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count-vectorizer\n",
    "[[394  98]\n",
    " [ 66 442]]\n",
    "accuracy_rate = 0.836\n",
    "\n",
    "# tf-idf vectorizer\n",
    "[[405  87]\n",
    " [ 67 441]]\n",
    "accuracy_rate = 0.846"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-fabric",
   "metadata": {},
   "source": [
    "Now let's try to not use stop words filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count-vectorizer\n",
    "[[413  70]\n",
    " [101 416]]\n",
    "accuracy_rate = 0.829\n",
    "\n",
    "# tf-idf vectorizer\n",
    "[[420  63]\n",
    " [103 414]]\n",
    "accuracy_rate = 0.834"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-vietnam",
   "metadata": {},
   "source": [
    "The remarkable inference is that when not using filtering out, we get a higher success rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-steal",
   "metadata": {},
   "source": [
    "Now let's end our writing with the last configuration containing too the first and third columns as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count-vectorizer\n",
    "[[416  78]\n",
    " [ 87 419]]\n",
    "accuracy_rate = 0.835\n",
    "\n",
    "# tf-idf vectorizer\n",
    "[[446  48]\n",
    " [125 381]]\n",
    "accuracy_rate = 0.827"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-professor",
   "metadata": {},
   "source": [
    "It doesn't give rise to a meaningful change in the success rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-update",
   "metadata": {},
   "source": [
    "Naive Bayes is a substantially fast algorithm that assumes the features are independent of each other. Despite the assumption seems not feasible, it really works well than expected. Being fast and easy to implement makes it a really good baseline performance algorithm. That is, when you want to develop a model, you may want to implement the first Naive Bayes to compare your actual algorithm against it in the future to decide how good your model is. And lastly, I want to emphasize that TF-IDF implementation works usually better than the normal count-vectorizer, but occasionally it results in relatively bad results. Thereby, we can infer that TF-IDF is more susceptible to the training data changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
